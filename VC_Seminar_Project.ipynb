{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8e9827-67c7-4df3-a33f-6b39b242b708",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db15d686ea9ac6920835a9486010698f",
     "grade": false,
     "grade_id": "cell-50142891b18d9a68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"seminarlogo.png\" class=\"center\" style=\"width:100%;\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50e015-a14b-4bcf-8796-45f88d5b1c93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70a38740a3ea3ef9e9f02091d93df34f",
     "grade": false,
     "grade_id": "cell-f8f56a83dc8fea6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color:navy\"> **Seminar Project:** </span> <span style=\"color:green\"> **Video Coding**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3ef06-b97e-4b31-9bdc-d5697b42b9a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2a2e8935fbd0a88c3ef01a1f3595805",
     "grade": false,
     "grade_id": "cell-b6c65d68479e9e79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style = \"color: navy\"> **General Procedure:** </span>\n",
    "1. Make group of $2$ or $3$ students while coordinating with each other and select one group coordinator.\n",
    "2. Select one the techniques (MDCT or Lapped Transform)\n",
    "4. Send us the the selected task (technique) and the list of group members with their <b>a)</b> Matriculation Numbers <b>b)</b> email IDs, and highlighting the group coordinator. Make it in a **table** form.\n",
    "5. The duration of the seminar project is from **11.06.2024** to **31.07.2024**, with a mid-term meeting with each group from *01.07.2024 to 06.07.2024* (Book an appointment at your convenience)\n",
    "6. There will be a report at the end of project. The submission date for the report is **31.07.2024**\n",
    "7. The report and results presentation will held from *01.08.2024 to 15.08.2024* (The specific date for each group will be selected after mutual consensus)\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> Note: Further Instructions and procedures may be updated from time to time</span>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ab026-535c-405a-873f-8eea172670db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cea22af77a68148302a9971e62f249b",
     "grade": false,
     "grade_id": "cell-3728701544a83df4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style = \"color: navy\"> **Introduction:** </span>\n",
    "There are two seminar projects, with the aim to compress the given images (Only use the given image in the base repository). The projects are:\n",
    "<br>\n",
    "### **Project-1:** <span style = \"color: green\"> Image-Compression using MDCT </span>\n",
    "### **Project-2:** <span style = \"color: green\"> Image-Compression using Lapped Transform </span>\n",
    "\n",
    "The projects are team-based work with a group of **2** (minimum) to **3** (maximum) students. You (as a team) are required to select one project and implement the technique (either *MDCT* or *Lapped* Transform). <span style = \"color: red\">To make a balance in project selection, we can assign either Project-1 or Project-2 to any group.</span>\n",
    "\n",
    "### <span style = \"color: navy\"> GET STARTED </span>\n",
    "The basic building blocks (base algorithm and/or material) of the Projects are provided in the git-repository (https://github.com/Karanraj06/image-compression).\n",
    "In this repository, the implementation of image compression is done by using simple **DCT** which you can take as your staring point and further develop or modify this base-repository according to the selected project i.e. techniques (*MDCT* or *Lapped* Transform) of selected project.\n",
    "\n",
    "### <span style = \"color: navy\"> Project Tasks </span>\n",
    "1. Take the base repository as base or reference\n",
    "2. Modify the DCT part with your selected technique (either MDCT or Lapped Transform)\n",
    "    - For the \"MDCT\" you need to find out the optimum window, try different windows\n",
    "    - The \"lapped transform\" needs to be implemented by Pytorch's Conv2D. For that, the kernel size can be chosen like the MDCT, which is 16x16. In order to get good transform filters, it needs to be trained, and for that you need to use the images in the image coder repository.\n",
    "5. Compare the results of compression of your modified version with the reference, which is DCT, by using Perceptual Similarity Metric and compression ratio. The repository for Perceptual Similarity Metric measurement is given here (https://github.com/richzhang/PerceptualSimilarity)\n",
    "6. The expected calculations among others should include the results of \"bits per pixel\"\n",
    "\n",
    "### <span style = \"color: navy\"> Some use full links </span>\n",
    "1. https://github.com/TUIlmenauAMS/Python-Audio-Coder\n",
    "2. https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "##### <span style=\"color:blue\"> Further details will be discussed with the group coordinators and the they are responsible for group coordination</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c732dd-751d-4e75-9801-dc9cbc89a373",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 01:** DCT </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c3d7c7-4682-4ba3-b107-5242a0c8a8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 64722, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a069fd-0528-470b-a343-1e35024bf2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyWavelets in /home/jupyter-64365/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3,>=1.22.4 in /opt/tljh/user/lib/python3.10/site-packages (from PyWavelets) (1.26.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /opt/tljh/user/lib/python3.10/site-packages (10.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install PyWavelets\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b904b6c3-e491-46eb-a811-e35023df398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import lpips\n",
    "import torch\n",
    "import pandas as pd\n",
    "import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3744e78-6bf3-49ed-a5e7-e71a237c7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from scipy.fftpack import dct, idct\n",
    "from torchvision import transforms\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2041c289-62e6-477b-9259-3167dfdd01b1",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d47826208e64049dc43b5b212c9fd01",
     "grade": true,
     "grade_id": "cell-e5b0ba0210863b90",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the paths\n",
    "path_color = path.PATH_COLOR\n",
    "path_gray = path.PATH_GRAY\n",
    "path_color_dct = path.PATH_COLOR_DCT\n",
    "path_gray_dct = path.PATH_GRAY_DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c3ffb3-2c69-427c-a41e-9905ca0c17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_color_dct, exist_ok=True)\n",
    "os.makedirs(path_gray_dct, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418dd2d0-0976-41ab-9339-88cae44699c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_file_paths(directory):\n",
    "    path = os.path.abspath(directory)\n",
    "    return [entry.path for entry in os.scandir(path) if entry.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1218993c-96ab-4f64-9cc0-602c5f3acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_color_images = absolute_file_paths(path_color)\n",
    "all_gray_images = absolute_file_paths(path_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c88abf-4594-4f02-bd76-8dac20e313f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_dct_images(image_paths, output_path):\n",
    "    for image_path in image_paths:\n",
    "        # Load image as grayscale\n",
    "        im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        f_image = np.float32(im)\n",
    "\n",
    "        # Apply DCT\n",
    "        dct_image = cv2.dct(f_image)\n",
    "\n",
    "        # Amplify high-frequency components (optional, adjust as needed)\n",
    "        dct_image *= 2  # Experiment with different values for quality adjustment\n",
    "\n",
    "        # Apply inverse DCT\n",
    "        idct_image = cv2.idct(dct_image)\n",
    "\n",
    "        # Clip values to valid range and convert back to uint8\n",
    "        idct_image_clipped = np.clip(idct_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Save the compressed image\n",
    "        filename = os.path.basename(image_path)\n",
    "        to_save = os.path.join(output_path, filename)\n",
    "        print(f\"Saving: {to_save}\")\n",
    "        cv2.imwrite(to_save, idct_image_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be36dc97-a745-46ea-985f-4ebcb7402b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/7.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/3.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/4.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/6.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/9.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/5.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/2.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/10.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/1.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/8.png\n"
     ]
    }
   ],
   "source": [
    "process_and_save_dct_images(all_color_images, path_color_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a3f9cb-ae2f-4094-9459-d9f8d61f4d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/3.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/5.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/9.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/8.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/1.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/2.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/4.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/7.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/6.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/10.tif\n"
     ]
    }
   ],
   "source": [
    "process_and_save_dct_images(all_gray_images, path_gray_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf07d1-a2b2-4523-8da6-b3e3c423e1c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 02:** LAPPED TRANSFORM </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b45696-f2d1-4125-b091-3e967f7e515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 64722, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38e7805-371a-4dec-a036-8740ce95cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the paths\n",
    "path_color = path.PATH_COLOR\n",
    "path_gray = path.PATH_GRAY\n",
    "path_color_lapped = path.PATH_COLOR_LAP\n",
    "path_gray_lapped = path.PATH_GRAY_LAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd84695-79fe-4d90-821f-529d44155abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_color_lapped, exist_ok=True)\n",
    "os.makedirs(path_gray_lapped, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e171b1-ad8f-495f-a1f3-15d3f17ec858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_file_paths(directory):\n",
    "    path = os.path.abspath(directory)\n",
    "    return [entry.path for entry in os.scandir(path) if entry.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f853040-33eb-4e85-ae31-415d0da6943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_color_images = absolute_file_paths(path_color)\n",
    "all_gray_images = absolute_file_paths(path_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a195d-9743-4671-9a79-f67654894654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "BLOCK_SIZE = 8\n",
    "OVERLAP = 4\n",
    "STEP_SIZE = BLOCK_SIZE - OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad3219b-57a0-483b-ad6e-e0b8be9dedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform DCT and IDCT with overlap\n",
    "def lapped_transform(im_np, block_size=BLOCK_SIZE, overlap=OVERLAP):\n",
    "    height, width = im_np.shape\n",
    "    step_size = block_size - overlap\n",
    "\n",
    "    # Padding the image\n",
    "    pad_height = (step_size - height % step_size) % step_size\n",
    "    pad_width = (step_size - width % step_size) % step_size\n",
    "    im_np_padded = np.pad(im_np, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Initialize arrays for compressed and decompressed data\n",
    "    compressed = np.zeros_like(im_np_padded, dtype=np.float32)\n",
    "    count = np.zeros_like(im_np_padded, dtype=np.float32)\n",
    "\n",
    "    # Process each block with overlap\n",
    "    for i in range(0, im_np_padded.shape[0] - overlap, step_size):\n",
    "        for j in range(0, im_np_padded.shape[1] - overlap, step_size):\n",
    "            block = im_np_padded[i:i + block_size, j:j + block_size]\n",
    "\n",
    "            # Apply DCT\n",
    "            dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "\n",
    "            # (Optional) Quantization and compression can be applied here\n",
    "\n",
    "            # Apply inverse DCT\n",
    "            idct_block = idct(idct(dct_block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "\n",
    "            # Overlap-add and average\n",
    "            compressed[i:i + block_size, j:j + block_size] += idct_block\n",
    "            count[i:i + block_size, j:j + block_size] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    count[count == 0] = 1\n",
    "    compressed /= count\n",
    "\n",
    "    # Clip values to valid range\n",
    "    compressed_clipped = np.clip(compressed, 0, 255).astype(np.uint8)\n",
    "    return compressed_clipped\n",
    "\n",
    "# Function to process and save Lapped Transform compressed images\n",
    "def process_and_save_lapped_transform_images(image_paths, output_path):\n",
    "    for image_path in image_paths:\n",
    "        # Load image as grayscale\n",
    "        im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        f_image = np.float32(im)\n",
    "\n",
    "        # Apply Lapped Transform\n",
    "        compressed_image = lapped_transform(f_image)\n",
    "\n",
    "        # Save the compressed image\n",
    "        filename = os.path.basename(image_path)\n",
    "        to_save = os.path.join(output_path, filename)\n",
    "        print(f\"Saving: {to_save}\")\n",
    "        cv2.imwrite(to_save, compressed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e5a736-f9de-4bec-aa2b-739e883ea0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/7.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/3.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/4.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/6.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/9.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/5.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/2.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/10.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/1.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/8.png\n"
     ]
    }
   ],
   "source": [
    "process_and_save_lapped_transform_images(all_color_images, path_color_lapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc35bc6-1e41-4e4c-8fcc-55f34829c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/3.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/5.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/9.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/8.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/1.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/2.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/4.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/7.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/6.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/10.tif\n"
     ]
    }
   ],
   "source": [
    "process_and_save_lapped_transform_images(all_gray_images, path_gray_lapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da4ca2-e86f-4a28-a057-ed79d5c9f4b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 03:** COMPRESSION RATIO </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf0ae2b-5be4-450e-bbac-d92cd5865951",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 64722, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40f0fdc7-ce89-4515-958b-a671fa3dc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_color = path.PATH_COLOR\n",
    "path_gray = path.PATH_GRAY\n",
    "path_color_dct = path.PATH_COLOR_DCT\n",
    "path_gray_dct = path.PATH_GRAY_DCT\n",
    "path_color_lapped = path.PATH_COLOR_LAP\n",
    "path_gray_lapped = path.PATH_GRAY_LAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46fa9957-096f-4887-9d32-850f09fba474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get file size\n",
    "def get_file_size(file_path):\n",
    "    return os.path.getsize(file_path) if os.path.isfile(file_path) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446e7dae-2a7a-4ba3-b1f4-79fde0e58efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate compression ratio for each image\n",
    "def calculate_compression_ratios(original_path, compressed_dct_path, compressed_lbt_path):\n",
    "    ratios = []\n",
    "    for filename in os.listdir(original_path):\n",
    "        original_image_path = os.path.join(original_path, filename)\n",
    "        dct_image_path = os.path.join(compressed_dct_path, filename)\n",
    "        lbt_image_path = os.path.join(compressed_lbt_path, filename)\n",
    "        \n",
    "        if not os.path.isfile(dct_image_path) or not os.path.isfile(lbt_image_path):\n",
    "            continue\n",
    "\n",
    "        # File sizes\n",
    "        original_size = get_file_size(original_image_path)\n",
    "        dct_size = get_file_size(dct_image_path)\n",
    "        lbt_size = get_file_size(lbt_image_path)\n",
    "\n",
    " # Compression ratios\n",
    "        ratio_dct = original_size / dct_size if dct_size > 0 else 0\n",
    "        ratio_lbt = original_size / lbt_size if lbt_size > 0 else 0\n",
    "\n",
    "        ratios.append({\n",
    "            'Filename': filename,\n",
    "            'Original Size (bytes)': original_size,\n",
    "            'DCT Size (bytes)': dct_size,\n",
    "            'Lapped Size (bytes)': lbt_size,\n",
    "            'DCT Compression Ratio': ratio_dct,\n",
    "            'Lapped Compression Ratio': ratio_lbt\n",
    "        })\n",
    "    \n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a00b9f0c-7fb5-433d-8fc7-606989a91fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Images Compression Ratios:\n",
      "  Filename  Original Size (bytes)  DCT Size (bytes)  Lapped Size (bytes)  \\\n",
      "0    7.png                 566322            177153               216321   \n",
      "1    3.png                 502888            179540               200815   \n",
      "2    4.png                 637432            220141               234133   \n",
      "3    6.png                 618959            181689               239507   \n",
      "4    9.png                 582899            117524               222787   \n",
      "5    5.png                 785610            291705               288386   \n",
      "6    2.png                 617995            261397               221222   \n",
      "7   10.png                 593463            174171               228518   \n",
      "8    1.png                 736501            235530               288012   \n",
      "9    8.png                 788470            241369               307807   \n",
      "\n",
      "   DCT Compression Ratio  Lapped Compression Ratio  \n",
      "0               3.196796                  2.617971  \n",
      "1               2.800980                  2.504235  \n",
      "2               2.895562                  2.722521  \n",
      "3               3.406695                  2.584304  \n",
      "4               4.959829                  2.616396  \n",
      "5               2.693166                  2.724161  \n",
      "6               2.364201                  2.793551  \n",
      "7               3.407358                  2.597008  \n",
      "8               3.126994                  2.557189  \n",
      "9               3.266658                  2.561573  \n",
      "\n",
      "Gray Images Compression Ratios:\n",
      "  Filename  Original Size (bytes)  DCT Size (bytes)  Lapped Size (bytes)  \\\n",
      "0    3.tif                 360678            302060               342816   \n",
      "1    5.tif                 286740            175354               202878   \n",
      "2    9.tif                 765450            267352               472874   \n",
      "3    8.tif                 356976            174060               240098   \n",
      "4    1.tif                  54968             12648                26068   \n",
      "5    2.tif                   1286              1860                 1930   \n",
      "6    4.tif                  89148              5892                 9824   \n",
      "7    7.tif                1049286             43038                43038   \n",
      "8    6.tif                 430068            128678               227610   \n",
      "9   10.tif                 360678            314220               340262   \n",
      "\n",
      "   DCT Compression Ratio  Lapped Compression Ratio  \n",
      "0               1.194061                  1.052104  \n",
      "1               1.635206                  1.413362  \n",
      "2               2.863079                  1.618719  \n",
      "3               2.050879                  1.486793  \n",
      "4               4.345984                  2.108639  \n",
      "5               0.691398                  0.666321  \n",
      "6              15.130346                  9.074511  \n",
      "7              24.380454                 24.380454  \n",
      "8               3.342203                  1.889495  \n",
      "9               1.147852                  1.060001  \n"
     ]
    }
   ],
   "source": [
    "# Calculate compression ratios for color and gray images\n",
    "color_ratios = calculate_compression_ratios(path_color, path_color_dct, path_color_lapped)\n",
    "gray_ratios = calculate_compression_ratios(path_gray, path_gray_dct, path_gray_lapped)\n",
    "\n",
    "# Create DataFrames to display results\n",
    "df_color = pd.DataFrame(color_ratios)\n",
    "df_gray = pd.DataFrame(gray_ratios)\n",
    "\n",
    "print(\"Color Images Compression Ratios:\")\n",
    "print(df_color)\n",
    "df_color.to_csv('/home/jupyter-64365/VC_Seminar_Project/color_com_ratio.csv', index=False)\n",
    "\n",
    "print(\"\\nGray Images Compression Ratios:\")\n",
    "print(df_gray)\n",
    "df_gray.to_csv('/home/jupyter-64365/VC_Seminar_Project/gray_com_ratio.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade02172-dece-4b9d-bb7c-6e07542a799b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 04:** LPIPS </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20a2cdcb-c137-479b-a045-11d3129a313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 64722, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0483d7c0-d4ea-429f-a92f-43f87dcc969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_color_dct = path.PATH_COLOR_DCT\n",
    "path_gray_dct = path.PATH_GRAY_DCT\n",
    "\n",
    "path_color_lapped = path.PATH_COLOR_LAP\n",
    "path_gray_lapped = path.PATH_GRAY_LAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "917a0e23-c8f0-4cdf-977a-913e747cdbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/tljh/user/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter-64365/.local/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex')  # Options: alex, vgg, squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecb8a930-f1ec-4c4c-bb40-64cdc262d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize for uniformity\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bbb5bb9-c713-424b-b860-dc1c81992660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate LPIPS for a set of images\n",
    "def calculate_lpips(original_path, compressed_path):\n",
    "    total_lpips = 0.0\n",
    "    image_count = 0\n",
    "    data = []\n",
    "    \n",
    "    for filename in os.listdir(original_path):\n",
    "        original_image_path = os.path.join(original_path, filename)\n",
    "        compressed_image_path = os.path.join(compressed_path, filename)\n",
    "\n",
    "        # Check if corresponding compressed image exists and skip directories\n",
    "        if not os.path.isfile(original_image_path) or not os.path.isfile(compressed_image_path):\n",
    "            continue\n",
    "\n",
    "        # Load images\n",
    "        original_image = Image.open(original_image_path).convert('RGB')\n",
    "        compressed_image = Image.open(compressed_image_path).convert('RGB')\n",
    "\n",
    "        # Transform images\n",
    "        original_tensor = transform(original_image).unsqueeze(0)\n",
    "        compressed_tensor = transform(compressed_image).unsqueeze(0)\n",
    "\n",
    "        # Calculate LPIPS\n",
    "        lpips_value = lpips_model(original_tensor, compressed_tensor)\n",
    "        lpips_value_item = lpips_value.item()\n",
    "        total_lpips += lpips_value_item\n",
    "        image_count += 1\n",
    "\n",
    "        #print(f\"LPIPS for {filename}: {lpips_value.item()}\")\n",
    "\n",
    "        # Store result\n",
    "        data.append({'File Name':filename, 'LPIPS Value':lpips_value_item})\n",
    "        \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "    \n",
    "    # Calculate average LPIPS\n",
    "    #avg_lpips = total_lpips / image_count if image_count > 0 else 0\n",
    "    #return avg_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a327436-e161-4b31-b92e-aeb2c95aa699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPIPS Scores for Color Images:\n",
      "  File Name  LPIPS Value\n",
      "0     7.png     0.337410\n",
      "1     3.png     0.311069\n",
      "2     4.png     0.295282\n",
      "3     6.png     0.315630\n",
      "4     9.png     0.373465\n",
      "5     5.png     0.211828\n",
      "6     2.png     0.320994\n",
      "7    10.png     0.416705\n",
      "8     1.png     0.222255\n",
      "9     8.png     0.272568\n"
     ]
    }
   ],
   "source": [
    "# Calculate LPIPS for color images\n",
    "lpips_color = calculate_lpips(path_color_dct, path_color_lapped)\n",
    "#print(f\"Average LPIPS for color images: {lpips_color}\")\n",
    "print(\"LPIPS Scores for Color Images:\")\n",
    "print(lpips_color)\n",
    "lpips_color.to_csv('/home/jupyter-64365/VC_Seminar_Project/color_lpips_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8018b5eb-dac5-4550-985f-ac876e88383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPIPS Scores for Gray Images:\n",
      "  File Name  LPIPS Value\n",
      "0     3.tif     0.200471\n",
      "1     5.tif     0.286067\n",
      "2     9.tif     0.104148\n",
      "3     8.tif     0.105291\n",
      "4     1.tif     0.832379\n",
      "5     2.tif     0.064019\n",
      "6     4.tif     0.595423\n",
      "7     7.tif     0.000004\n",
      "8     6.tif     0.119508\n",
      "9    10.tif     0.229499\n"
     ]
    }
   ],
   "source": [
    "# Calculate LPIPS for gray images\n",
    "lpips_gray = calculate_lpips(path_gray_dct, path_gray_lapped)\n",
    "#print(f\"Average LPIPS for gray images: {lpips_gray}\")\n",
    "print(\"LPIPS Scores for Gray Images:\")\n",
    "print(lpips_gray)\n",
    "lpips_gray.to_csv('/home/jupyter-64365/VC_Seminar_Project/gray_lpips_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da85af-347b-40b8-bf31-5a05bca167c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dc50680a95fdd1d5a311ed82e8fbb2a",
     "grade": false,
     "grade_id": "cell-c6e9ec1a38c4bda2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> END Report here </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
