{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8e9827-67c7-4df3-a33f-6b39b242b708",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7d8e9827-67c7-4df3-a33f-6b39b242b708",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db15d686ea9ac6920835a9486010698f",
     "grade": false,
     "grade_id": "cell-50142891b18d9a68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<img src=\"seminarlogo.png\" class=\"center\" style=\"width:100%;\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50e015-a14b-4bcf-8796-45f88d5b1c93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0c50e015-a14b-4bcf-8796-45f88d5b1c93",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70a38740a3ea3ef9e9f02091d93df34f",
     "grade": false,
     "grade_id": "cell-f8f56a83dc8fea6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# <span style=\"color:navy\"> **Seminar Project:** </span> <span style=\"color:green\"> **Video Coding**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3ef06-b97e-4b31-9bdc-d5697b42b9a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7ee3ef06-b97e-4b31-9bdc-d5697b42b9a2",
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2a2e8935fbd0a88c3ef01a1f3595805",
     "grade": false,
     "grade_id": "cell-b6c65d68479e9e79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style = \"color: navy\"> **General Procedure:** </span>\n",
    "1. Make group of $2$ or $3$ students while coordinating with each other and select one group coordinator.\n",
    "2. Select one the techniques (MDCT or Lapped Transform)\n",
    "4. Send us the the selected task (technique) and the list of group members with their <b>a)</b> Matriculation Numbers <b>b)</b> email IDs, and highlighting the group coordinator. Make it in a **table** form.\n",
    "5. The duration of the seminar project is from **11.06.2024** to **31.07.2024**, with a mid-term meeting with each group from *01.07.2024 to 06.07.2024* (Book an appointment at your convenience)\n",
    "6. There will be a report at the end of project. The submission date for the report is **31.07.2024**\n",
    "7. The report and results presentation will held from *01.08.2024 to 15.08.2024* (The specific date for each group will be selected after mutual consensus)\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> Note: Further Instructions and procedures may be updated from time to time</span>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ab026-535c-405a-873f-8eea172670db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a78ab026-535c-405a-873f-8eea172670db",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cea22af77a68148302a9971e62f249b",
     "grade": false,
     "grade_id": "cell-3728701544a83df4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## <span style = \"color: navy\"> **Introduction:** </span>\n",
    "There are two seminar projects, with the aim to compress the given images (Only use the given image in the base repository). The projects are:\n",
    "<br>\n",
    "### **Project-1:** <span style = \"color: green\"> Image-Compression using MDCT </span>\n",
    "### **Project-2:** <span style = \"color: green\"> Image-Compression using Lapped Transform </span>\n",
    "\n",
    "The projects are team-based work with a group of **2** (minimum) to **3** (maximum) students. You (as a team) are required to select one project and implement the technique (either *MDCT* or *Lapped* Transform). <span style = \"color: red\">To make a balance in project selection, we can assign either Project-1 or Project-2 to any group.</span>\n",
    "\n",
    "### <span style = \"color: navy\"> GET STARTED </span>\n",
    "The basic building blocks (base algorithm and/or material) of the Projects are provided in the git-repository (https://github.com/Karanraj06/image-compression).\n",
    "In this repository, the implementation of image compression is done by using simple **DCT** which you can take as your staring point and further develop or modify this base-repository according to the selected project i.e. techniques (*MDCT* or *Lapped* Transform) of selected project.\n",
    "\n",
    "### <span style = \"color: navy\"> Project Tasks </span>\n",
    "1. Take the base repository as base or reference\n",
    "2. Modify the DCT part with your selected technique (either MDCT or Lapped Transform)\n",
    "    - For the \"MDCT\" you need to find out the optimum window, try different windows\n",
    "    - The \"lapped transform\" needs to be implemented by Pytorch's Conv2D. For that, the kernel size can be chosen like the MDCT, which is 16x16. In order to get good transform filters, it needs to be trained, and for that you need to use the images in the image coder repository.\n",
    "5. Compare the results of compression of your modified version with the reference, which is DCT, by using Perceptual Similarity Metric and compression ratio. The repository for Perceptual Similarity Metric measurement is given here (https://github.com/richzhang/PerceptualSimilarity)\n",
    "6. The expected calculations among others should include the results of \"bits per pixel\"\n",
    "\n",
    "### <span style = \"color: navy\"> Some use full links </span>\n",
    "1. https://github.com/TUIlmenauAMS/Python-Audio-Coder\n",
    "2. https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "##### <span style=\"color:blue\"> Further details will be discussed with the group coordinators and the they are responsible for group coordination</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c732dd-751d-4e75-9801-dc9cbc89a373",
   "metadata": {
    "id": "02c732dd-751d-4e75-9801-dc9cbc89a373",
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 01:** DCT </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c3d7c7-4682-4ba3-b107-5242a0c8a8a5",
   "metadata": {
    "id": "43c3d7c7-4682-4ba3-b107-5242a0c8a8a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 64722, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a069fd-0528-470b-a343-1e35024bf2f6",
   "metadata": {
    "id": "10a069fd-0528-470b-a343-1e35024bf2f6",
    "outputId": "02549af3-a7dc-4a43-b402-48f9360ecee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyWavelets in /home/jupyter-64365/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3,>=1.22.4 in /opt/tljh/user/lib/python3.10/site-packages (from PyWavelets) (1.26.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /opt/tljh/user/lib/python3.10/site-packages (10.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in /home/jupyter-64365/.local/lib/python3.10/site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/tljh/user/lib/python3.10/site-packages (from scikit-image) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/tljh/user/lib/python3.10/site-packages (from scikit-image) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/tljh/user/lib/python3.10/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/tljh/user/lib/python3.10/site-packages (from scikit-image) (10.0.1)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/jupyter-64365/.local/lib/python3.10/site-packages (from scikit-image) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/jupyter-64365/.local/lib/python3.10/site-packages (from scikit-image) (2024.5.3)\n",
      "Requirement already satisfied: packaging>=21 in /opt/tljh/user/lib/python3.10/site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/jupyter-64365/.local/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install PyWavelets\n",
    "!pip install pillow\n",
    "!apt install lpips\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b904b6c3-e491-46eb-a811-e35023df398d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 618,
     "status": "error",
     "timestamp": 1722435263665,
     "user": {
      "displayName": "Artun Altınel",
      "userId": "18269428795213045283"
     },
     "user_tz": -120
    },
    "id": "b904b6c3-e491-46eb-a811-e35023df398d",
    "outputId": "52d0fc08-8876-4a84-e640-d78774588d4c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import lpips\n",
    "import torch\n",
    "import pandas as pd\n",
    "import path\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3744e78-6bf3-49ed-a5e7-e71a237c7d74",
   "metadata": {
    "id": "d3744e78-6bf3-49ed-a5e7-e71a237c7d74"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from scipy.fftpack import dct, idct\n",
    "from torchvision import transforms\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2041c289-62e6-477b-9259-3167dfdd01b1",
   "metadata": {
    "id": "2041c289-62e6-477b-9259-3167dfdd01b1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d47826208e64049dc43b5b212c9fd01",
     "grade": true,
     "grade_id": "cell-e5b0ba0210863b90",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the paths\n",
    "path_color = path.PATH_COLOR\n",
    "path_gray = path.PATH_GRAY\n",
    "path_color_dct = path.PATH_COLOR_DCT\n",
    "path_gray_dct = path.PATH_GRAY_DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c3ffb3-2c69-427c-a41e-9905ca0c17be",
   "metadata": {
    "id": "00c3ffb3-2c69-427c-a41e-9905ca0c17be"
   },
   "outputs": [],
   "source": [
    "os.makedirs(path_color_dct, exist_ok=True)\n",
    "os.makedirs(path_gray_dct, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418dd2d0-0976-41ab-9339-88cae44699c3",
   "metadata": {
    "id": "418dd2d0-0976-41ab-9339-88cae44699c3"
   },
   "outputs": [],
   "source": [
    "def absolute_file_paths(directory):\n",
    "    path = os.path.abspath(directory)\n",
    "    return [entry.path for entry in os.scandir(path) if entry.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1218993c-96ab-4f64-9cc0-602c5f3acac5",
   "metadata": {
    "id": "1218993c-96ab-4f64-9cc0-602c5f3acac5"
   },
   "outputs": [],
   "source": [
    "all_color_images = absolute_file_paths(path_color)\n",
    "all_gray_images = absolute_file_paths(path_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c88abf-4594-4f02-bd76-8dac20e313f8",
   "metadata": {
    "id": "50c88abf-4594-4f02-bd76-8dac20e313f8"
   },
   "outputs": [],
   "source": [
    "def process_and_save_dct_images(image_paths, output_path):\n",
    "    for image_path in image_paths:\n",
    "        # Load image as grayscale\n",
    "        im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        f_image = np.float32(im)\n",
    "\n",
    "        # Apply DCT\n",
    "        dct_image = cv2.dct(f_image)\n",
    "\n",
    "        # Amplify high-frequency components (optional, adjust as needed)\n",
    "        dct_image *= 2  # Experiment with different values for quality adjustment\n",
    "\n",
    "        # Apply inverse DCT\n",
    "        idct_image = cv2.idct(dct_image)\n",
    "\n",
    "        # Clip values to valid range and convert back to uint8\n",
    "        idct_image_clipped = np.clip(idct_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Save the compressed image\n",
    "        filename = os.path.basename(image_path)\n",
    "        to_save = os.path.join(output_path, filename)\n",
    "        print(f\"Saving: {to_save}\")\n",
    "        cv2.imwrite(to_save, idct_image_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be36dc97-a745-46ea-985f-4ebcb7402b1f",
   "metadata": {
    "id": "be36dc97-a745-46ea-985f-4ebcb7402b1f",
    "outputId": "4ffff9f9-bbfd-4cba-daa5-63ad11a8af6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/7.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/3.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/4.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/6.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/9.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/5.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/2.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/10.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/1.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-dct/8.png\n"
     ]
    }
   ],
   "source": [
    "process_and_save_dct_images(all_color_images, path_color_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a3f9cb-ae2f-4094-9459-d9f8d61f4d3e",
   "metadata": {
    "id": "40a3f9cb-ae2f-4094-9459-d9f8d61f4d3e",
    "outputId": "9eb4a06f-ad40-4af1-f802-a0476c3ab2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/3.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/5.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/9.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/8.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/1.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/2.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/4.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/7.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/6.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-dct/10.tif\n"
     ]
    }
   ],
   "source": [
    "process_and_save_dct_images(all_gray_images, path_gray_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf07d1-a2b2-4523-8da6-b3e3c423e1c6",
   "metadata": {
    "id": "6baf07d1-a2b2-4523-8da6-b3e3c423e1c6",
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 02:** LAPPED TRANSFORM </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b45696-f2d1-4125-b091-3e967f7e515f",
   "metadata": {
    "id": "03b45696-f2d1-4125-b091-3e967f7e515f"
   },
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 64722';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38e7805-371a-4dec-a036-8740ce95cf10",
   "metadata": {
    "id": "b38e7805-371a-4dec-a036-8740ce95cf10"
   },
   "outputs": [],
   "source": [
    "# Access the paths\n",
    "path_color = path.PATH_COLOR\n",
    "path_gray = path.PATH_GRAY\n",
    "path_color_lapped = path.PATH_COLOR_LAP\n",
    "path_gray_lapped = path.PATH_GRAY_LAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd84695-79fe-4d90-821f-529d44155abe",
   "metadata": {
    "id": "6fd84695-79fe-4d90-821f-529d44155abe"
   },
   "outputs": [],
   "source": [
    "os.makedirs(path_color_lapped, exist_ok=True)\n",
    "os.makedirs(path_gray_lapped, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e171b1-ad8f-495f-a1f3-15d3f17ec858",
   "metadata": {
    "id": "58e171b1-ad8f-495f-a1f3-15d3f17ec858"
   },
   "outputs": [],
   "source": [
    "def absolute_file_paths(directory):\n",
    "    path = os.path.abspath(directory)\n",
    "    return [entry.path for entry in os.scandir(path) if entry.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f853040-33eb-4e85-ae31-415d0da6943e",
   "metadata": {
    "id": "4f853040-33eb-4e85-ae31-415d0da6943e"
   },
   "outputs": [],
   "source": [
    "all_color_images = absolute_file_paths(path_color)\n",
    "all_gray_images = absolute_file_paths(path_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766a195d-9743-4671-9a79-f67654894654",
   "metadata": {
    "id": "766a195d-9743-4671-9a79-f67654894654"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "BLOCK_SIZE = 8\n",
    "OVERLAP = 4\n",
    "STEP_SIZE = BLOCK_SIZE - OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad3219b-57a0-483b-ad6e-e0b8be9dedab",
   "metadata": {
    "id": "6ad3219b-57a0-483b-ad6e-e0b8be9dedab"
   },
   "outputs": [],
   "source": [
    "# Function to perform DCT and IDCT with overlap\n",
    "def lapped_transform(im_np, block_size=BLOCK_SIZE, overlap=OVERLAP):\n",
    "    height, width = im_np.shape\n",
    "    step_size = block_size - overlap\n",
    "\n",
    "    # Padding the image\n",
    "    pad_height = (step_size - height % step_size) % step_size\n",
    "    pad_width = (step_size - width % step_size) % step_size\n",
    "    im_np_padded = np.pad(im_np, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Initialize arrays for compressed and decompressed data\n",
    "    compressed = np.zeros_like(im_np_padded, dtype=np.float32)\n",
    "    count = np.zeros_like(im_np_padded, dtype=np.float32)\n",
    "\n",
    "    # Process each block with overlap\n",
    "    for i in range(0, im_np_padded.shape[0] - overlap, step_size):\n",
    "        for j in range(0, im_np_padded.shape[1] - overlap, step_size):\n",
    "            block = im_np_padded[i:i + block_size, j:j + block_size]\n",
    "\n",
    "            # Apply DCT\n",
    "            dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "\n",
    "            # (Optional) Quantization and compression can be applied here\n",
    "\n",
    "            # Apply inverse DCT\n",
    "            idct_block = idct(idct(dct_block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "\n",
    "            # Overlap-add and average\n",
    "            compressed[i:i + block_size, j:j + block_size] += idct_block\n",
    "            count[i:i + block_size, j:j + block_size] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    count[count == 0] = 1\n",
    "    compressed /= count\n",
    "\n",
    "    # Clip values to valid range\n",
    "    compressed_clipped = np.clip(compressed, 0, 255).astype(np.uint8)\n",
    "    return compressed_clipped\n",
    "\n",
    "# Function to process and save Lapped Transform compressed images\n",
    "def process_and_save_lapped_transform_images(image_paths, output_path):\n",
    "    for image_path in image_paths:\n",
    "        # Load image as grayscale\n",
    "        im = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        f_image = np.float32(im)\n",
    "\n",
    "        # Apply Lapped Transform\n",
    "        compressed_image = lapped_transform(f_image)\n",
    "\n",
    "        # Save the compressed image\n",
    "        filename = os.path.basename(image_path)\n",
    "        to_save = os.path.join(output_path, filename)\n",
    "        print(f\"Saving: {to_save}\")\n",
    "        cv2.imwrite(to_save, compressed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e5a736-f9de-4bec-aa2b-739e883ea0c6",
   "metadata": {
    "id": "64e5a736-f9de-4bec-aa2b-739e883ea0c6",
    "outputId": "6067a60c-836c-4204-c642-3e74e142c8f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/7.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/3.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/4.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/6.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/9.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/5.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/2.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/10.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/1.png\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/color-lapped/8.png\n"
     ]
    }
   ],
   "source": [
    "process_and_save_lapped_transform_images(all_color_images, path_color_lapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc35bc6-1e41-4e4c-8fcc-55f34829c29b",
   "metadata": {
    "id": "8dc35bc6-1e41-4e4c-8fcc-55f34829c29b",
    "outputId": "6b1676f5-9366-4e19-b88c-dd553becd717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/3.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/5.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/9.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/8.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/1.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/2.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/4.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/7.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/6.tif\n",
      "Saving: /home/jupyter-64365/VC_Seminar_Project/gray-lapped/10.tif\n"
     ]
    }
   ],
   "source": [
    "process_and_save_lapped_transform_images(all_gray_images, path_gray_lapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da4ca2-e86f-4a28-a057-ed79d5c9f4b6",
   "metadata": {
    "id": "d7da4ca2-e86f-4a28-a057-ed79d5c9f4b6",
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 03:** COMPRESSION RATIO </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf0ae2b-5be4-450e-bbac-d92cd5865951",
   "metadata": {
    "id": "6cf0ae2b-5be4-450e-bbac-d92cd5865951"
   },
   "outputs": [],
   "source": [
    "MatriculationNumber = '64722, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40f0fdc7-ce89-4515-958b-a671fa3dc895",
   "metadata": {
    "id": "40f0fdc7-ce89-4515-958b-a671fa3dc895"
   },
   "outputs": [],
   "source": [
    "path_color = path.PATH_COLOR\n",
    "path_gray = path.PATH_GRAY\n",
    "path_color_dct = path.PATH_COLOR_DCT\n",
    "path_gray_dct = path.PATH_GRAY_DCT\n",
    "path_color_lapped = path.PATH_COLOR_LAP\n",
    "path_gray_lapped = path.PATH_GRAY_LAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46fa9957-096f-4887-9d32-850f09fba474",
   "metadata": {
    "id": "46fa9957-096f-4887-9d32-850f09fba474"
   },
   "outputs": [],
   "source": [
    "# Function to get file size\n",
    "def get_file_size(file_path):\n",
    "    return os.path.getsize(file_path) if os.path.isfile(file_path) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446e7dae-2a7a-4ba3-b1f4-79fde0e58efb",
   "metadata": {
    "id": "446e7dae-2a7a-4ba3-b1f4-79fde0e58efb"
   },
   "outputs": [],
   "source": [
    "# Function to calculate compression ratio for each image\n",
    "def calculate_compression_ratios(original_path, compressed_dct_path, compressed_lbt_path):\n",
    "    ratios = []\n",
    "    for filename in os.listdir(original_path):\n",
    "        original_image_path = os.path.join(original_path, filename)\n",
    "        dct_image_path = os.path.join(compressed_dct_path, filename)\n",
    "        lbt_image_path = os.path.join(compressed_lbt_path, filename)\n",
    "\n",
    "        if not os.path.isfile(dct_image_path) or not os.path.isfile(lbt_image_path):\n",
    "            continue\n",
    "\n",
    "        # File sizes\n",
    "        original_size = get_file_size(original_image_path)\n",
    "        dct_size = get_file_size(dct_image_path)\n",
    "        lbt_size = get_file_size(lbt_image_path)\n",
    "\n",
    " # Compression ratios\n",
    "        if dct_size < original_size : \n",
    "            ratio_dct = (100 -((dct_size*100)/original_size)) \n",
    "        else:   \n",
    "            ratio_dct = (100 -((dct_size*100)/original_size)) \n",
    "        if lbt_size < original_size: \n",
    "            ratio_lbt = (100 -((lbt_size*100)/original_size)) \n",
    "        else:   \n",
    "            ratio_lbt = (100 -((lbt_size*100)/original_size))\n",
    "        ratios.append({\n",
    "            'Filename': filename,\n",
    "            'Original Size (bytes)': original_size,\n",
    "            'DCT Size (bytes)': dct_size,\n",
    "            'LT Size (bytes)': lbt_size,\n",
    "            'DCT Comp Ratio %': ratio_dct,\n",
    "            'Lapped Comp Ratio %': ratio_lbt\n",
    "        })\n",
    "\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a00b9f0c-7fb5-433d-8fc7-606989a91fa3",
   "metadata": {
    "id": "a00b9f0c-7fb5-433d-8fc7-606989a91fa3",
    "outputId": "109f35af-0820-4f09-e641-930c4bb08fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Images Compression Ratios:\n",
      "  Filename  Original Size (bytes)  DCT Size (bytes)  LT Size (bytes)  \\\n",
      "0    7.png                 566322            177153           216321   \n",
      "1    3.png                 502888            179540           200815   \n",
      "2    4.png                 637432            220141           234133   \n",
      "3    6.png                 618959            181689           239507   \n",
      "4    9.png                 582899            117524           222787   \n",
      "5    5.png                 785610            291705           288386   \n",
      "6    2.png                 617995            261397           221222   \n",
      "7   10.png                 593463            174171           228518   \n",
      "8    1.png                 736501            235530           288012   \n",
      "9    8.png                 788470            241369           307807   \n",
      "\n",
      "   DCT Comp Ratio %  Lapped Comp Ratio %  \n",
      "0         68.718679            61.802473  \n",
      "1         64.298214            60.067649  \n",
      "2         65.464395            63.269337  \n",
      "3         70.646036            61.304868  \n",
      "4         79.838017            61.779485  \n",
      "5         62.868981            63.291455  \n",
      "6         57.702409            64.203270  \n",
      "7         70.651751            61.494145  \n",
      "8         68.020410            60.894554  \n",
      "9         69.387675            60.961482  \n",
      "\n",
      "Gray Images Compression Ratios:\n",
      "  Filename  Original Size (bytes)  DCT Size (bytes)  LT Size (bytes)  \\\n",
      "0    3.tif                 360678            302060           342816   \n",
      "1    5.tif                 286740            175354           202878   \n",
      "2    9.tif                 765450            267352           472874   \n",
      "3    8.tif                 356976            174060           240098   \n",
      "4    1.tif                  54968             12648            26068   \n",
      "5    2.tif                   1286              1860             1930   \n",
      "6    4.tif                  89148              5892             9824   \n",
      "7    7.tif                1049286             43038            43038   \n",
      "8    6.tif                 430068            128678           227610   \n",
      "9   10.tif                 360678            314220           340262   \n",
      "\n",
      "   DCT Comp Ratio %  Lapped Comp Ratio %  \n",
      "0         16.252170             4.952340  \n",
      "1         38.845644            29.246704  \n",
      "2         65.072572            38.222745  \n",
      "3         51.240420            32.741137  \n",
      "4         76.990249            52.576044  \n",
      "5        -44.634526           -50.077760  \n",
      "6         93.390766            88.980123  \n",
      "7         95.898354            95.898354  \n",
      "8         70.079615            47.075811  \n",
      "9         12.880741             5.660451  \n"
     ]
    }
   ],
   "source": [
    "# Calculate compression ratios for color and gray images\n",
    "color_ratios = calculate_compression_ratios(path_color, path_color_dct, path_color_lapped)\n",
    "gray_ratios = calculate_compression_ratios(path_gray, path_gray_dct, path_gray_lapped)\n",
    "\n",
    "# Create DataFrames to display results\n",
    "df_color = pd.DataFrame(color_ratios)\n",
    "df_gray = pd.DataFrame(gray_ratios)\n",
    "\n",
    "print(\"Color Images Compression Ratios:\")\n",
    "print(df_color)\n",
    "df_color.to_csv(path.PATH_COLOR_RATIO, index=False)\n",
    "\n",
    "print(\"\\nGray Images Compression Ratios:\")\n",
    "print(df_gray)\n",
    "df_gray.to_csv(path.PATH_GRAY_RATIO, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade02172-dece-4b9d-bb7c-6e07542a799b",
   "metadata": {
    "id": "ade02172-dece-4b9d-bb7c-6e07542a799b",
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#ebeff7; padding:1.25em; border-radius:1em; border: 1px solid black\">\n",
    "\n",
    "### <span style=\"color:navy; font-size: 1.25em\"> **TASK 04:** LPIPS </span>\n",
    "#### <span style=\"color:blue\"> Below Enter Matriculation Numbers of each member </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20a2cdcb-c137-479b-a045-11d3129a313f",
   "metadata": {
    "id": "20a2cdcb-c137-479b-a045-11d3129a313f"
   },
   "outputs": [],
   "source": [
    "MatriculationNumber = '64365, 66160';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0483d7c0-d4ea-429f-a92f-43f87dcc969a",
   "metadata": {
    "id": "0483d7c0-d4ea-429f-a92f-43f87dcc969a"
   },
   "outputs": [],
   "source": [
    "path_color_dct = path.PATH_COLOR_DCT\n",
    "path_gray_dct = path.PATH_GRAY_DCT\n",
    "\n",
    "path_color_lapped = path.PATH_COLOR_LAP\n",
    "path_gray_lapped = path.PATH_GRAY_LAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "917a0e23-c8f0-4cdf-977a-913e747cdbf9",
   "metadata": {
    "id": "917a0e23-c8f0-4cdf-977a-913e747cdbf9",
    "outputId": "6d327751-392b-49b6-c99e-ed443ab2e841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/tljh/user/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter-64365/.local/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex')  # Options: alex, vgg, squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecb8a930-f1ec-4c4c-bb40-64cdc262d355",
   "metadata": {
    "id": "ecb8a930-f1ec-4c4c-bb40-64cdc262d355"
   },
   "outputs": [],
   "source": [
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize for uniformity\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bbb5bb9-c713-424b-b860-dc1c81992660",
   "metadata": {
    "id": "2bbb5bb9-c713-424b-b860-dc1c81992660"
   },
   "outputs": [],
   "source": [
    "# Function to calculate LPIPS for a set of images\n",
    "def calculate_lpips(original_path, compressed_path):\n",
    "    total_lpips = 0.0\n",
    "    image_count = 0\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(original_path):\n",
    "        original_image_path = os.path.join(original_path, filename)\n",
    "        compressed_image_path = os.path.join(compressed_path, filename)\n",
    "\n",
    "        # Check if corresponding compressed image exists and skip directories\n",
    "        if not os.path.isfile(original_image_path) or not os.path.isfile(compressed_image_path):\n",
    "            continue\n",
    "\n",
    "        # Load images\n",
    "        original_image = Image.open(original_image_path).convert('RGB')\n",
    "        compressed_image = Image.open(compressed_image_path).convert('RGB')\n",
    "\n",
    "        # Transform images\n",
    "        original_tensor = transform(original_image).unsqueeze(0)\n",
    "        compressed_tensor = transform(compressed_image).unsqueeze(0)\n",
    "\n",
    "        # Calculate LPIPS\n",
    "        lpips_value = lpips_model(original_tensor, compressed_tensor)\n",
    "        lpips_value_item = lpips_value.item()*100\n",
    "        total_lpips += lpips_value_item\n",
    "        image_count += 1\n",
    "\n",
    "        #print(f\"LPIPS for {filename}: {lpips_value.item()}\")\n",
    "\n",
    "        # Store result\n",
    "        data.append({'File Name':filename, 'LPIPS Value (Percentage Of Diff%)':lpips_value_item})\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "    # Calculate average LPIPS\n",
    "    #avg_lpips = total_lpips / image_count if image_count > 0 else 0\n",
    "    #return avg_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a327436-e161-4b31-b92e-aeb2c95aa699",
   "metadata": {
    "id": "9a327436-e161-4b31-b92e-aeb2c95aa699",
    "outputId": "c46b7e13-39c8-4167-c897-f9e62e90c5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPIPS Scores for Color Images DCT:\n",
      "  File Name  LPIPS Value (Percentage Of Diff%)\n",
      "0     7.png                          52.786756\n",
      "1     3.png                          53.720617\n",
      "2     4.png                          59.807819\n",
      "3     6.png                          46.721593\n",
      "4     9.png                          44.856182\n",
      "5     5.png                          43.165866\n",
      "6     2.png                          75.108820\n",
      "7    10.png                          50.046206\n",
      "8     1.png                          43.696877\n",
      "9     8.png                          36.130676\n",
      "LPIPS Scores for Color Images Lapped :\n",
      "  File Name  LPIPS Value (Percentage Of Diff%)\n",
      "0     7.png                          34.117484\n",
      "1     3.png                          36.033899\n",
      "2     4.png                          42.898655\n",
      "3     6.png                          24.276394\n",
      "4     9.png                          19.903517\n",
      "5     5.png                          28.031820\n",
      "6     2.png                          58.564758\n",
      "7    10.png                          19.571954\n",
      "8     1.png                          28.234750\n",
      "9     8.png                          13.924675\n"
     ]
    }
   ],
   "source": [
    "# Calculate LPIPS for color images\n",
    "lpips_color = calculate_lpips(path_color, path_color_dct)\n",
    "#print(f\"Average LPIPS for color images: {lpips_color}\")\n",
    "print(\"LPIPS Scores for Color Images DCT:\")\n",
    "print(lpips_color)\n",
    "lpips_color.to_csv(path.PATH_COLOR_LPIPS, index=False)\n",
    "\n",
    "# Calculate LPIPS for color images\n",
    "lpips_color = calculate_lpips(path_color, path_color_lapped)\n",
    "#print(f\"Average LPIPS for color images: {lpips_color}\")\n",
    "print(\"LPIPS Scores for Color Images Lapped :\")\n",
    "print(lpips_color)\n",
    "lpips_color.to_csv(path.PATH_GRAY_LPIPS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8018b5eb-dac5-4550-985f-ac876e88383a",
   "metadata": {
    "id": "8018b5eb-dac5-4550-985f-ac876e88383a",
    "outputId": "207707de-7ab8-4a2a-be67-a2c888031af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPIPS Scores for Gray Images DCT:\n",
      "  File Name  LPIPS Value (Percentage Of Diff%)\n",
      "0     3.tif                          19.953492\n",
      "1     5.tif                          28.491843\n",
      "2     9.tif                           9.121278\n",
      "3     8.tif                          10.662258\n",
      "4     1.tif                          79.879838\n",
      "5     2.tif                           6.394510\n",
      "6     4.tif                          59.455323\n",
      "7     7.tif                           0.000000\n",
      "8     6.tif                          11.840390\n",
      "9    10.tif                          22.912861\n",
      "LPIPS Scores for Gray Images Lapped:\n",
      "  File Name  LPIPS Value (Percentage Of Diff%)\n",
      "0     3.tif                           0.007921\n",
      "1     5.tif                           0.032098\n",
      "2     9.tif                           1.900384\n",
      "3     8.tif                           0.549822\n",
      "4     1.tif                           3.133356\n",
      "5     2.tif                           0.001509\n",
      "6     4.tif                           0.010007\n",
      "7     7.tif                           0.000426\n",
      "8     6.tif                           0.010335\n",
      "9    10.tif                           0.012815\n"
     ]
    }
   ],
   "source": [
    "# Calculate LPIPS for gray images\n",
    "lpips_gray = calculate_lpips(path_gray, path_gray_dct)\n",
    "#print(f\"Average LPIPS for gray images: {lpips_gray}\")\n",
    "print(\"LPIPS Scores for Gray Images DCT:\")\n",
    "print(lpips_gray)\n",
    "lpips_gray.to_csv('/home/jupyter-64365/VC_Seminar_Project/gray_lpips_results_dct.csv', index=False)\n",
    "\n",
    "# Calculate LPIPS for gray images\n",
    "lpips_gray = calculate_lpips(path_gray, path_gray_lapped)\n",
    "#print(f\"Average LPIPS for gray images: {lpips_gray}\")\n",
    "print(\"LPIPS Scores for Gray Images Lapped:\")\n",
    "print(lpips_gray)\n",
    "lpips_gray.to_csv('/home/jupyter-64365/VC_Seminar_Project/gray_lpips_results_Lapped.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da85af-347b-40b8-bf31-5a05bca167c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "43da85af-347b-40b8-bf31-5a05bca167c4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dc50680a95fdd1d5a311ed82e8fbb2a",
     "grade": false,
     "grade_id": "cell-c6e9ec1a38c4bda2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> END Report here </span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
